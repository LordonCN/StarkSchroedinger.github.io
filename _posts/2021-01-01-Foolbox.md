---
layout:     post
title:      "Foolbox: A Python toolbox to benchmark the robustness of machine learning models"
subtitle:   "论文精度笔记"
date:       2021-01-01
author:     "Felix Zhang"
header-img: "img/in-post/2021-01-01-Foolbox/bg.JPG"
catalog: true
tags:
   - Deep Learning
   - Adversarial Machine Learning
   - Paper Notes
---

Foolbox给我们提供了一个专门用来产生攻击并评价不同ML模型对攻击鲁棒性的工具箱。这个工具箱提供了对不同深度学习框架以及不同的评价标准的适配。

机器的预测可能会因为某些细小的改动而产生极大的差异，这事和人脑不同的地方，这就是所谓的Adversarial Attack。为了量化系统对这种攻击的鲁棒性，Szegedy在提出了基于average size of the minimum adversarial perturbation across many examples的评价标准后，焦点落在了如何寻找global minimum adversarial purturbation上。现有的办法基本上都是对样本实施尽可能多的攻击，并且在所有有效攻击中取平均，但是现在大部分实施攻击的方法都受限于——不同平台、框架下的可实施性和可比性。与现有的cleverhans相比，Foolbox的是那个新特性：

* 和现有的使用最多的深度学习框架都做了相应的适配。
* 提供了超过15种攻击手段的使用范例。
* 对不同的攻击都提供了不同的评价手段。

作者在文章的第一部分介绍了Foolbox所适配的深度学习框架、支持攻击的不同评价方式、对攻击的不同量化标准、攻击测评结果。

在第二部分重点介绍了在Foolbox中使用的攻击手段：

* Gradiant-Based Attacks

  * Gradient Attack

  * Gradiant Sign Attack

  * Iterative Gradient Attack

  * Iterative Gradient Sign Attack

  * DeepFool L2 Attack

  * DeepFool L-Infinite Attack

  * L-BFGS Attack
  * SLSQP Attack
  * Jacobian-Based Saliency Map Attack

* Score-Based Attacks

  * Single Pixel Attack
  * Local Search Attack
  * Approximate L-BFGS Attack

* Decision-Based Attacks

  * Boundary Attack
  * Pointwise Attack
  * Additive Uniform Noise Attack
  * Additive Gaussian Noise Attack
  * Salt and Pepper Noise Attack
  * Contrast Reduction Attack
  * Gaussian Blur Attack
  * Precomputed Images Attack

